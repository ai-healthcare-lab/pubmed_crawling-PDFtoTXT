{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어를 입력하세요:depression\n",
      "몇 페이지까지 크롤링할까요? :1\n",
      "-----1페이지 결과입니다.-----\n",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6801385/pdf/ijms-20-04827.pdf\n",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4790405/pdf/CN-13-494.pdf\n",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3998225/pdf/1471-244X-14-107.pdf\n",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7381373/pdf/nihms-1600792.pdf\n",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4997396/pdf/nutrients-08-00483.pdf\n",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5613659/pdf/nihms875825.pdf\n",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5532074/pdf/nihms856948.pdf\n",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4790400/pdf/CN-13-480.pdf\n",
      "C:/crawling/pdf_1.pdf이 저장되었습니다.\n",
      "C:/crawling/pdf_2.pdf이 저장되었습니다.\n",
      "C:/crawling/pdf_3.pdf이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LTTextBoxHorizontal\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request   \n",
    "import urllib.parse\n",
    "import requests\n",
    "\n",
    "    \n",
    "    \n",
    "def SearchDocument(Search_word, LastPage):\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}\n",
    "\n",
    "    pageNum = 1  \n",
    "    count = 0  \n",
    "    lst_2 = []\n",
    "    \n",
    "    while pageNum < LastPage + 1:  \n",
    "        \n",
    "        lst = []\n",
    "\n",
    "        url = f'https://pubmed.ncbi.nlm.nih.gov/?term={Search_word}&filter=simsearch2.ffrft&page={pageNum}'   \n",
    "        html = urllib.request.urlopen(url).read()         \n",
    "        soup = BeautifulSoup(html, 'html.parser')        \n",
    "        title = soup.find_all(class_='docsum-title')     \n",
    "        print(f'-----{pageNum}페이지 결과입니다.-----')\n",
    "        \n",
    "        for firstpage in title:\n",
    "            sub_url = \"https://pubmed.ncbi.nlm.nih.gov\" + firstpage.attrs['href']    \n",
    "            sub_resp = requests.get(sub_url, headers = headers)\n",
    "            sub_soup = BeautifulSoup(sub_resp.text, 'html5lib')\n",
    "            sub_sel = \"div.full-view > div.full-text-links-list > a\"  \n",
    "            sub_titles = sub_soup.select(sub_sel)\n",
    "\n",
    "            for secondpage in sub_titles:\n",
    "#                 print(j)                                            # journal 이름 알 수 있음\n",
    "                hypertext = secondpage.attrs['href'].split('/')  \n",
    "                    \n",
    "                for key_word in hypertext:\n",
    "                    if 'pmc' in key_word:                            # hypertext 속에서 원하는 journal의 논문 찾기위해 키워드를 입력한다.                  \n",
    "                        lst.append(secondpage.attrs['href'])\n",
    "                                                \n",
    "        for m in range(len(lst)):\n",
    "\n",
    "            ssub_url = lst[m]\n",
    "            req = Request(ssub_url, headers=headers)\n",
    "            webpage = urlopen(req).read()\n",
    "            soup = BeautifulSoup(webpage, 'html.parser')   \n",
    "            sel = soup.select(\"div.format-menu > ul > li > a\")\n",
    "\n",
    "            for hypertext_2 in sel:\n",
    "                if \".pdf\" in hypertext_2.attrs['href']:\n",
    "                    hypertext_2 = hypertext_2.attrs['href']\n",
    "                    downloadlink=(\"https://www.ncbi.nlm.nih.gov\"+hypertext_2)\n",
    "                    lst_2.append(downloadlink)           \n",
    "                    print(downloadlink)\n",
    "            \n",
    "        pageNum += 1\n",
    "        \n",
    "    return lst_2\n",
    "\n",
    "\n",
    "# pdf 다운 / SearchDocument의 return 값인 다운로드 링크들이 담긴 리스트를 받는다.    \n",
    "def Downloader(List):\n",
    "    count = 0\n",
    "    lst = []\n",
    "    \n",
    "    for k in range(len(List)):\n",
    "        pdf_url = List[k]\n",
    "        count += 1\n",
    "        save_name = \"C:/crawling/pdf_\" + str(count) +\".pdf\"     # pdf 저장경로입니다, count를 이용해서 이름을 붙였습니다.\n",
    "        \n",
    "        download_file = requests.get(pdf_url, headers={'User-Agent' : 'Mozilla/5.0 (Windows NT 6.1: Win64: x64) AppleWebKit/537.36 (KHTML, Like Gecko) Chrome/73.0.3683.86 Safari/537.36'})\n",
    "        \n",
    "        document = open(save_name, 'wb')\n",
    "        document.write(download_file.content)\n",
    "        document.close()\n",
    "        \n",
    "        lst.append(save_name)\n",
    "        print(save_name + \"이 저장되었습니다.\")\n",
    "    print(\"다운로드가 끝났습니다.\")\n",
    "    return lst\n",
    "\n",
    "# 변환 / Downloader를 통해 return 값인 리스트로 저장된 주소들을 받아서 동작한다.\n",
    "def parsedocument(List):\n",
    "    lines = []\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    laparams = LAParams()\n",
    "    device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    for k in List:                                             # 여기서 k는 pdf 저장경로입니다. \n",
    "        fh = open (k,'rb') \n",
    "            \n",
    "        lines = []\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "\n",
    "        for page in PDFPage.get_pages(fh):\n",
    "                interpreter.process_page(page)\n",
    "                layout = device.get_result()\n",
    "                for element in layout:\n",
    "                    if isinstance(element, LTTextBoxHorizontal):\n",
    "                        if 70 < element.bbox[1] < 745 and element.bbox[2] > 33:  # x1,x2,y1,y2 좌표를 이용해 불필요한 영역을 제거한다\n",
    "                            lines.append(element.get_text())\n",
    "\n",
    "        for i in range(len(lines)):\n",
    "            lines[i]=lines[i].strip()\n",
    "            lines[i]=lines[i].replace('-\\n','')\n",
    "            lines[i]=lines[i].replace('\\n',' ')\n",
    "            lines[i]=lines[i].replace('  ',' ')\n",
    "\n",
    "        lines=' '.join(lines)\n",
    "        \n",
    "        for ab in ['abstract', 'Abstract', 'ABSTRACT']:\n",
    "            if ab in lines:\n",
    "                index = lines.find(ab)\n",
    "                pass\n",
    "            \n",
    "        for ack in ['acknowledgements', 'Acknowledgements', 'ACKNOWLEDGEMENTS','acknowledgments', 'Acknowledgments', 'ACKNOWLEDGMENTS']:\n",
    "            if ack in lines:\n",
    "                index_2 = lines.rfind(ack)\n",
    "                break\n",
    "            else:\n",
    "                for ref in ['references','References','REFERENCES']:\n",
    "                    if ref in lines:\n",
    "                        index_2 = lines.rfind(ref)\n",
    "                        pass \n",
    "                    \n",
    "        result = lines[index:index_2]\n",
    "            \n",
    "        with open(k[:-4]+'.txt', mode='w', encoding='UTF-8') as output:    # 저장경로, 편의를 위해서 전 count를 매개변수로 받아 사용하였습니다.\n",
    "            output.write(result)\n",
    "            \n",
    "        print(k+\"가 변환되었습니다.\")                                      # k == pdf 저장경로\n",
    "    print(\"변환이 끝났습니다.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    searchWord = urllib.parse.quote_plus(input('검색어를 입력하세요:'))  # 검색할 단어\n",
    "    pageNum = int(input('몇 페이지까지 크롤링할까요? :'))   # 몇 페이지까지 검색할지 설정\n",
    "    \n",
    "    SearchList = SearchDocument(searchWord, pageNum)  \n",
    "    DownloadList = Downloader(SearchList)\n",
    "    parsedocument(DownloadList)\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
